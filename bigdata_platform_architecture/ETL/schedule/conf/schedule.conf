#!/bin/sh

# 启动时加载配置
# hadoop client程序配置
HADOOP_ENV=/home/hadoop/hadoop-2.6.4
HADOOP_CLASSPATH="$HADOOP_ENV/share/hadoop/tools/lib/"
alias hadoop_command="$HADOOP_ENV/bin/hadoop"
alias hadoop_streaming="$HADOOP_ENV/bin/hadoop jar $HADOOP_ENV/share/hadoop/tools/lib/hadoop-streaming-2.6.4.jar "

# 工作目录
WORK_DIR=$(pwd)

# log file
LOG_FILE=${WORK_DIR}/log/schedule.log
if [ ! -d ${WORK_DIR}/log ];then
    mkdir -p ${WORK_DIR}/log
    touch ${LOG_FILE}
fi

# source dir: flume的输出目录
SOURCE_DIR_PREFIX=/usr/dash/flume/spool/

# mapred output dir: 日志解析和抽取后的结果存储路径
MAPRED_OUTPUT_DIR_PREFIX=/usr/dash/output/

# 原始日志删除的时间间隔，单位：天
SOURCE_DEL_INTERVAL=3

# 判断输出是否完整的条件, 下期改为MySQL行数查询方式
READY_FILE_NUM=1

# hive db
HIVE_DB=log_db

# hive table array, MR任务结束后，循环load进不同的表
HIVE_TABLE_ARRAY=(log_player log_view)

# hive table partition删除的时间间隔，单位:天
PARTITION_DEL_INTERVAL=7

# mapred job name prefix
JOB_NAME_PREFIX=log_etl

# map script
MAP_SCRIPT="../extractor/common/lighttpd_log_parser.awk"

# reduce script
REDUCE_SCRIPT="python ../extractor/Extractor.py

# map 任务使用的程序压缩包的HDFS路径，必须先上传到HDFS
MAP_ARCHIVE_PATH="/usr/dash/archive/extractor-1.0.0.tar.gz#extractor"

